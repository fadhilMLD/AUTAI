script1 = r'''
print("""
---pppppp
 Generated by Autai
 Made by Fadhil
 - Feaven Technologies - 

 Lisence - Absolutely no restriction for any kind of purpose

 Ensure you use these model for ease of development 
 without hurting or commiting any sort of criminal activity.


 █████╗ ██╗   ██╗████████╗ █████╗ ██╗
██╔══██╗██║   ██║╚══██╔══╝██╔══██╗██║
███████║██║   ██║   ██║   ███████║██║
██╔══██║██║   ██║   ██║   ██╔══██║██║
██║  ██║╚██████╔╝   ██║   ██║  ██║██║
╚═╝  ╚═╝ ╚═════╝    ╚═╝   ╚═╝  ╚═╝╚═╝



>>> Select the CSV file containing your dataset when prompted <<<
""")
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
from tkinter import Tk, filedialog
import numpy as np

def select_csv_file():
    root = Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="Select CSV file",
        filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
    )
    return file_path

# CSV
csv_file = select_csv_file()
if not csv_file:
    print("No file selected. Exiting...")
    exit()

df = pd.read_csv(csv_file)
print("\nColumns available in the dataset:\n", list(df.columns))

'''

script2 = r'''

def get_save_directory():
    """Open a directory dialog to select where to save the model"""
    root = Tk()
    root.withdraw()
    root.attributes('-topmost', True)  # Bring dialog to front
    
    folder_path = filedialog.askdirectory(
        title="Select folder to save trained model"
    )
    
    root.destroy()
    return folder_path

def plot_loss_terminal(epoch_losses, width=60, height=15):
    """Plot loss graph in terminal using ASCII characters"""
    if not epoch_losses:
        return
    
    min_loss = min(epoch_losses)
    max_loss = max(epoch_losses)
    loss_range = max_loss - min_loss if max_loss != min_loss else 1
    
    # Create graph grid
    graph = [[' ' for _ in range(width)] for _ in range(height)]
    
    # Plot data points
    for i, loss in enumerate(epoch_losses):
        x = int((i / (len(epoch_losses) - 1)) * (width - 1)) if len(epoch_losses) > 1 else 0
        y = height - 1 - int(((loss - min_loss) / loss_range) * (height - 1))
        if 0 <= x < width and 0 <= y < height:
            graph[y][x] = '●'
    
    # Draw connecting lines
    for i in range(len(epoch_losses) - 1):
        x1 = int((i / (len(epoch_losses) - 1)) * (width - 1))
        x2 = int(((i + 1) / (len(epoch_losses) - 1)) * (width - 1))
        loss1 = epoch_losses[i]
        loss2 = epoch_losses[i + 1]
        y1 = height - 1 - int(((loss1 - min_loss) / loss_range) * (height - 1))
        y2 = height - 1 - int(((loss2 - min_loss) / loss_range) * (height - 1))
        
        # Draw line between points
        if x1 != x2:
            for x in range(x1 + 1, x2):
                progress = (x - x1) / (x2 - x1)
                y = int(y1 + (y2 - y1) * progress)
                if 0 <= y < height and graph[y][x] == ' ':
                    graph[y][x] = '─'
    
    # Print graph
    print("\n" + "="*70)
    print("Training Loss vs Epoch".center(70))
    print("="*70)
    print(f"Max Loss: {max_loss:.6f}  │")
    
    for row_idx, row in enumerate(graph):
        # Calculate loss value for this row
        loss_val = max_loss - (row_idx / (height - 1)) * loss_range
        print(f"{loss_val:>10.4f}  │ {''.join(row)}")
    
    print(f"Min Loss: {min_loss:.6f}  │")
    print(" " * 13 + "└" + "─" * width)
    print(" " * 14 + "0" + " " * (width - len(str(len(epoch_losses)))) + str(len(epoch_losses)))
    print(" " * (width // 2) + "Epoch")
    print("="*70 + "\n")

def train_model(ModelClass, input_size, output_size, epochs=20, batch_size=8, lr=0.001):
    dataset = TensorDataset(X, y)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = ModelClass(input_size, output_size)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Setup for loss tracking
    epoch_losses = []
    
    print("\n" + "="*70)
    print("Starting Training".center(70))
    print("="*70 + "\n")

    for epoch in range(epochs):
        running_loss = 0.0
        batch_count = 0

        for inputs, labels in loader:
            optimizer.zero_grad()
            outputs = model(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            batch_count += 1

        # Calculate epoch loss
        epoch_loss = running_loss / len(loader)
        epoch_losses.append(epoch_loss)
        
        # Print progress bar
        progress = (epoch + 1) / epochs
        bar_length = 40
        filled_length = int(bar_length * progress)
        bar = '█' * filled_length + '░' * (bar_length - filled_length)
        
        print(f"\rEpoch [{epoch+1:>3}/{epochs}] {bar} {progress*100:>5.1f}% | Loss: {epoch_loss:.6f}", end='')
        
        # Plot graph every 5 epochs or on last epoch
        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
            print()  # New line before graph
            plot_loss_terminal(epoch_losses)
    
    print("\n" + "="*70)
    print("Training Complete!".center(70))
    print(f"Final Loss: {epoch_losses[-1]:.6f}".center(70))
    print("="*70 + "\n")

    # Get save directory from user
    save_dir = get_save_directory()
    
    if save_dir:  # If user didn't cancel the dialog
        model_path_pth = f"{save_dir}/trained_model.pth"
        model_path_pt = f"{save_dir}/trained_model_full.pt"
        
        torch.save(model.state_dict(), model_path_pth)
        torch.save(model, model_path_pt)
        
        print(f"✓ Model saved as '{model_path_pth}' and '{model_path_pt}'")
        
        # Save loss history as CSV
        loss_csv_path = f"{save_dir}/loss_history.csv"
        loss_df = pd.DataFrame({
            'epoch': list(range(1, len(epoch_losses) + 1)),
            'loss': epoch_losses
        })
        loss_df.to_csv(loss_csv_path, index=False)
        print(f"✓ Loss history saved as '{loss_csv_path}'")
    else:
        print("Save operation cancelled by user.")

    return model


if __name__ == "__main__":
    input_size = X.shape[1]
    output_size = y.shape[1] if len(y.shape) > 1 else 1
    trained_model = train_model(CustomModel, input_size, output_size, epochs=10)
'''



def topological_sort(inputt, conn):
    """Perform topological sort to get correct execution order"""
    # Build graph and calculate in-degrees
    graph = {layer['id']: [] for layer in inputt}
    in_degree = {layer['id']: 0 for layer in inputt}
    
    for connection in conn:
        graph[connection['from']].append(connection['to'])
        in_degree[connection['to']] += 1
    
    # Find starting nodes (input layers or nodes with no incoming edges)
    queue = []
    for layer in inputt:
        if layer['type'] == 'InputLayer' or in_degree[layer['id']] == 0:
            queue.append(layer['id'])
    
    # Perform topological sort
    sorted_order = []
    while queue:
        current_id = queue.pop(0)
        sorted_order.append(current_id)
        
        for neighbor in graph[current_id]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
    
    return sorted_order

def find_layer(id, dic):
    for i in dic:
        if i['id'] == id:
            return i

def calculate_layer_sizes(inputt, conn, sorted_order):
    """Calculate input and output sizes for all layers"""
    size_map = {}
    
    for layer_id in sorted_order:
        layer = find_layer(layer_id, inputt)
        
        if layer['type'] == 'InputLayer':
            size_map[layer_id] = layer['parameters']['inputShape'][0]
            
        elif layer['type'] == 'LinearLayer':
            # Find ALL input connections to determine input size
            input_connections = [c for c in conn if c['to'] == layer_id]
            
            if input_connections:
                total_input_size = 0
                for connection in input_connections:
                    prev_layer_id = connection['from']
                    prev_layer = find_layer(prev_layer_id, inputt)
                    
                    if prev_layer['type'] == 'LinearLayer':
                        total_input_size += size_map[prev_layer_id]['out']
                    elif prev_layer['type'] in ['InputLayer', 'ConcatLayer']:
                        if prev_layer_id in size_map:
                            if isinstance(size_map[prev_layer_id], dict):
                                total_input_size += size_map[prev_layer_id]['out']
                            else:
                                total_input_size += size_map[prev_layer_id]
                    # If we can't determine size, use default
                    else:
                        total_input_size = 64
                        break
                
                input_size = total_input_size
            else:
                input_size = 64  # Default if no connections
                
            output_size = layer['parameters']['outputSize']
            
            # Store both input and output sizes
            size_map[layer_id] = {
                'in': input_size,
                'out': output_size
            }
            
        elif layer['type'] == 'ConcatLayer':
            # Calculate total size from all inputs
            input_connections = [c for c in conn if c['to'] == layer_id]
            total_size = 0
            for connection in input_connections:
                input_layer = find_layer(connection['from'], inputt)
                if input_layer['type'] == 'LinearLayer':
                    total_size += size_map[connection['from']]['out']
                elif input_layer['type'] in ['InputLayer', 'ConcatLayer']:
                    if connection['from'] in size_map:
                        if isinstance(size_map[connection['from']], dict):
                            total_size += size_map[connection['from']]['out']
                        else:
                            total_size += size_map[connection['from']]
            size_map[layer_id] = total_size
            
        elif layer['type'] == 'OutputLayer':
            # Output size is fixed
            size_map[layer_id] = layer['parameters']['outputShape'][0]
    
    return size_map

def gen_script(inputt, conn, training_options=None):
    global script1, script2

    # Get topological order
    sorted_order = topological_sort(inputt, conn)
    
    # Calculate layer sizes
    size_map = calculate_layer_sizes(inputt, conn, sorted_order)
    
    # Build connection map for easier lookup
    connection_map = {}
    for connection in conn:
        if connection['to'] not in connection_map:
            connection_map[connection['to']] = []
        connection_map[connection['to']].append(connection['from'])
    
    # Assign unique variable names to all layers
    layer_vars = {}
    layer_counter = 0
    
    A = ''  # Layer definitions
    B_lines = ['    def forward(self, x):']  # Forward pass lines
    
    # Track input splits
    input_layers = [layer for layer in inputt if layer['type'] == 'InputLayer']
    total_input_size = sum(layer['parameters']['inputShape'][0] for layer in input_layers)
    
    print(f"DEBUG: Total input size: {total_input_size}")
    print(f"DEBUG: Input layers: {[(layer['id'], layer['parameters']['inputShape'][0]) for layer in input_layers]}")
    print(f"DEBUG: Size map: {size_map}")
    
    current_input_start = 0
    
    # Process layers in topological order
    for layer_id in sorted_order:
        layer = find_layer(layer_id, inputt)
        
        if layer['type'] == 'InputLayer':
            # Handle input layer
            input_size = layer['parameters']['inputShape'][0]
            var_name = f'input_{layer_counter}'
            layer_vars[layer_id] = var_name
            layer_counter += 1
            
            # Calculate correct slicing
            B_lines.append(f"        {var_name} = x[:, {current_input_start}:{current_input_start + input_size}]")
            current_input_start += input_size
            print(f"DEBUG: Input layer {layer_id} -> {var_name} = x[{current_input_start - input_size}:{current_input_start}]")
                
        elif layer['type'] == 'LinearLayer':
            # Create Linear layer
            var_name = f'linear_{layer_counter}'
            layer_counter += 1
            
            # Get sizes from calculated size_map
            if layer_id in size_map and isinstance(size_map[layer_id], dict):
                input_size = size_map[layer_id]['in']
                output_size = size_map[layer_id]['out']
            else:
                # Fallback to parameters if not in size_map
                input_size = layer['parameters']['inputSize']
                output_size = layer['parameters']['outputSize']
            
            A += f"        self.{var_name} = nn.Linear({input_size}, {output_size})\n"
            
            # Find input for this layer - handle multiple inputs by concatenating
            if layer_id in connection_map:
                input_sources = connection_map[layer_id]
                print(f"DEBUG: Linear layer {layer_id} has inputs: {input_sources}")
                
                if len(input_sources) == 1:
                    # Single input
                    input_source_id = input_sources[0]
                    if input_source_id in layer_vars:
                        input_var = layer_vars[input_source_id]
                        B_lines.append(f"        {var_name}_out = self.{var_name}({input_var})")
                    else:
                        B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                else:
                    # Multiple inputs - need to concatenate them first
                    concat_vars = []
                    for input_source_id in input_sources:
                        if input_source_id in layer_vars:
                            concat_vars.append(layer_vars[input_source_id])
                        else:
                            # If input source not processed yet, use x as fallback
                            concat_vars.append('x')
                    
                    if len(concat_vars) > 1:
                        # Multiple inputs, concatenate them
                        concat_var_name = f'concat_for_{var_name}'
                        concat_vars_str = ', '.join(concat_vars)
                        B_lines.append(f"        {concat_var_name} = torch.cat([{concat_vars_str}], dim=1)")
                        B_lines.append(f"        {var_name}_out = self.{var_name}({concat_var_name})")
                    elif len(concat_vars) == 1:
                        # Only one input after filtering
                        B_lines.append(f"        {var_name}_out = self.{var_name}({concat_vars[0]})")
                    else:
                        # No inputs found
                        B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                
                # Add activation function
                activation = layer['parameters'].get('activation', '')
                if activation == 'relu':
                    B_lines.append(f"        {var_name}_out = torch.relu({var_name}_out)")
                elif activation == 'sigmoid':
                    B_lines.append(f"        {var_name}_out = torch.sigmoid({var_name}_out)")
                elif activation == 'tanh':
                    B_lines.append(f"        {var_name}_out = torch.tanh({var_name}_out)")
                elif activation == 'leaky_relu':
                    B_lines.append(f"        {var_name}_out = torch.nn.functional.leaky_relu({var_name}_out)")
                
                layer_vars[layer_id] = f"{var_name}_out"
            else:
                # No connections, use raw input
                B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                layer_vars[layer_id] = f"{var_name}_out"
                
        elif layer['type'] == 'ConcatLayer':
            # Handle concatenation
            var_name = f'concat_{layer_counter}'
            layer_counter += 1
            
            # Find all inputs to this concat layer
            if layer_id in connection_map:
                input_vars = []
                for input_id in connection_map[layer_id]:
                    if input_id in layer_vars:
                        input_vars.append(layer_vars[input_id])
                    else:
                        input_vars.append('x')  # Fallback
                
                if len(input_vars) > 1:
                    concat_vars = ', '.join(input_vars)
                    B_lines.append(f"        {var_name} = torch.cat([{concat_vars}], dim=1)")
                    layer_vars[layer_id] = var_name
                elif len(input_vars) == 1:
                    layer_vars[layer_id] = input_vars[0]
                else:
                    layer_vars[layer_id] = 'x'
            else:
                layer_vars[layer_id] = 'x'
                
        elif layer['type'] == 'OutputLayer':
            # Create output layer
            var_name = f'output_{layer_counter}'
            layer_counter += 1
            
            # Find input size dynamically from size_map
            input_size = 64  # Default
            if layer_id in connection_map:
                input_source_id = connection_map[layer_id][0]
                if input_source_id in size_map:
                    input_layer = find_layer(input_source_id, inputt)
                    if input_layer['type'] == 'LinearLayer':
                        input_size = size_map[input_source_id]['out']
                    elif input_layer['type'] in ['ConcatLayer', 'InputLayer']:
                        input_size = size_map[input_source_id]
            
            output_size = layer['parameters']['outputShape'][0]
            A += f"        self.{var_name} = nn.Linear({input_size}, {output_size})\n"

            if layer_id in connection_map:
                input_source_id = connection_map[layer_id][0]
                if input_source_id in layer_vars:
                    input_var = layer_vars[input_source_id]
                    B_lines.append(f"        {var_name}_out = self.{var_name}({input_var})")
                    
                     
                    activation = layer['parameters'].get('activationFunction', '')
                    if output_size > 1 and activation == 'softmax':
                        B_lines.append(f"        {var_name}_out = torch.softmax({var_name}_out, dim=1)")
                    elif activation == 'relu':
                        B_lines.append(f"        {var_name}_out = torch.relu({var_name}_out)")
                    elif activation == 'sigmoid':
                        B_lines.append(f"        {var_name}_out = torch.sigmoid({var_name}_out)")
                    elif activation == 'tanh':
                        B_lines.append(f"        {var_name}_out = torch.tanh({var_name}_out)")
                    
                    layer_vars[layer_id] = f"{var_name}_out"
                else:
                    B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                    layer_vars[layer_id] = f"{var_name}_out"
            else:
                B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                layer_vars[layer_id] = f"{var_name}_out"
    
    output_layers = [layer for layer in inputt if layer['type'] == 'OutputLayer']
    if output_layers:
        output_id = output_layers[0]['id']
        if output_id in layer_vars:
            B_lines.append(f"        return {layer_vars[output_id]}")
        else:
            B_lines.append("        return x")
    else:
        B_lines.append("        return x")
    
    B = '\n'.join(B_lines)
    
    # Generate input column selection code for each input layer
    input_column_code = ""
    input_layers = [layer for layer in inputt if layer['type'] == 'InputLayer']
    
    # Find output layer to check number of output neurons
    output_layers = [layer for layer in inputt if layer['type'] == 'OutputLayer']
    num_output_neurons = output_layers[0]['parameters']['outputShape'][0] if output_layers else 1
    
    if len(input_layers) == 1:
        # Single input layer - use improved approach with validation
        input_size = input_layers[0]['parameters']['inputShape'][0]
        input_column_code = f'''
# Input column selection with validation
while True:
    input_cols_input = input("\\nEnter input column names (comma separated): ").strip()
    
    if input_cols_input:  # If user entered something
        input_cols = [col.strip() for col in input_cols_input.split(",")]
        # Filter only columns that exist in the dataframe
        input_cols = [col for col in input_cols if col in df.columns]
        
        if input_cols:
            break
        else:
            print("None of the entered columns exist in the dataset. Please try again.")
            print("Available columns:", list(df.columns))
    else:
        # If user pressed Enter without input, use all numeric columns except the label
        print("No columns specified. Using all numeric columns as input features.")
        import numpy as np
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        input_cols = numeric_cols
        break

print(f"Using {{len(input_cols)}} input columns")

# Ensure we have enough columns
if len(input_cols) < {input_size}:
    print(f"Warning: Need {input_size} input columns but only got {{len(input_cols)}}. Using first {input_size} available columns.")
    input_cols = input_cols[:{input_size}]
elif len(input_cols) > {input_size}:
    print(f"Warning: Expected {input_size} input columns but got {{len(input_cols)}}. Using first {input_size} columns.")
    input_cols = input_cols[:{input_size}]
'''
    else:
        # Multiple input layers - ask for each separately
        input_column_code = '''
# Input column selection for multiple input layers
print("\\n=== Input Column Selection ===")
import numpy as np
input_cols = []
'''
        for i, input_layer in enumerate(input_layers):
            layer_id = input_layer['id']
            input_size = input_layer['parameters']['inputShape'][0]
            input_column_code += f'''
while True:
    print(f"\\nInput Layer {i+1} (ID: {layer_id}) - needs {input_size} input columns")
    layer_{layer_id}_input = input(f"Enter {input_size} column names for Input Layer {i+1} (comma separated): ").strip()
    
    if layer_{layer_id}_input:
        layer_{layer_id}_cols = [col.strip() for col in layer_{layer_id}_input.split(",")]
        # Filter only existing columns
        layer_{layer_id}_cols = [col for col in layer_{layer_id}_cols if col in df.columns]
        
        if layer_{layer_id}_cols:
            break
        else:
            print("None of the entered columns exist in the dataset. Please try again.")
            print("Available columns:", list(df.columns))
    else:
        # Auto-select numeric columns if user pressed Enter
        print("No columns specified. Auto-selecting numeric columns.")
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        layer_{layer_id}_cols = numeric_cols
        break

# Ensure correct number of columns
if len(layer_{layer_id}_cols) < {input_size}:
    print(f"Warning: Need {input_size} columns but only got {{len(layer_{layer_id}_cols)}}. Using first {input_size} available.")
    layer_{layer_id}_cols = layer_{layer_id}_cols[:{input_size}]
elif len(layer_{layer_id}_cols) > {input_size}:
    print(f"Warning: Expected {input_size} columns but got {{len(layer_{layer_id}_cols)}}. Using first {input_size}.")
    layer_{layer_id}_cols = layer_{layer_id}_cols[:{input_size}]

input_cols.extend(layer_{layer_id}_cols)
'''

    # Add label column selection with validation and automatic one-hot encoding
    input_column_code += f'''
# Label column selection with validation and automatic one-hot encoding
while True:
    label_cols_input = input("\\nEnter label column names (comma separated): ").strip()
    
    if label_cols_input:
        label_cols = [col.strip() for col in label_cols_input.split(",")]
        # Filter only columns that exist in the dataframe
        label_cols = [col for col in label_cols if col in df.columns]
        
        if label_cols:
            break
        else:
            print("None of the entered label columns exist in the dataset. Please try again.")
            print("Available columns:", list(df.columns))
    else:
        print("Error: You must specify at least one label column.")
        print("Available columns:", list(df.columns))

print(f"Using {{len(label_cols)}} label columns")

# Check if we need to one-hot encode
# One-hot encode if: single label column AND multiple output neurons ({num_output_neurons} > 1)
if len(label_cols) == 1 and {num_output_neurons} > 1:
    print(f"\\nDetected single label column with {num_output_neurons} output neurons.")
    print("Automatically applying one-hot encoding to label column...")
    
    label_col_name = label_cols[0]
    unique_labels = df[label_col_name].unique()
    num_unique = len(unique_labels)
    
    print(f"Found {{num_unique}} unique classes in '{{label_col_name}}': {{unique_labels}}")
    
    if num_unique != {num_output_neurons}:
        print(f"Warning: Number of unique classes ({{num_unique}}) doesn't match output neurons ({num_output_neurons})")
        print(f"Using {num_output_neurons} output neurons as specified in the model architecture.")
    
    # Create one-hot encoded DataFrame
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder
    import warnings
    warnings.filterwarnings('ignore')
    
    # Encode labels to integers first
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(df[label_col_name])
    
    # One-hot encode
    onehot_encoder = OneHotEncoder(sparse_output=False)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))
    
    # If one-hot encoding produces more columns than output neurons, truncate
    if onehot_encoded.shape[1] > {num_output_neurons}:
        print(f"Truncating one-hot encoded output from {{onehot_encoded.shape[1]}} to {num_output_neurons} columns")
        onehot_encoded = onehot_encoded[:, :{num_output_neurons}]
    # If fewer columns, pad with zeros
    elif onehot_encoded.shape[1] < {num_output_neurons}:
        print(f"Padding one-hot encoded output from {{onehot_encoded.shape[1]}} to {num_output_neurons} columns")
        padding = np.zeros((onehot_encoded.shape[0], {num_output_neurons} - onehot_encoded.shape[1]))
        onehot_encoded = np.concatenate([onehot_encoded, padding], axis=1)
    
    # Create new DataFrame with one-hot columns
    onehot_df = pd.DataFrame(
        onehot_encoded, 
        columns=[f'{{label_col_name}}_class_{{i}}' for i in range({num_output_neurons})]
    )
    
    # Use one-hot encoded columns as labels
    label_cols = list(onehot_df.columns)
    df = pd.concat([df, onehot_df], axis=1)
    
    print(f"One-hot encoding complete. New label columns: {{label_cols}}")
    print(f"Label mapping: {{dict(enumerate(label_encoder.classes_))}}")

# Create tensors with error handling
try:
    X = torch.tensor(df[input_cols].values, dtype=torch.float32)
    y = torch.tensor(df[label_cols].values, dtype=torch.float32)
    
    print(f"\\nInput tensor shape: {{X.shape}}")
    print(f"Label tensor shape: {{y.shape}}")
    
    # Check for NaN or infinite values
    if torch.isnan(X).any() or torch.isinf(X).any():
        print("Warning: Input data contains NaN or infinite values")
    if torch.isnan(y).any() or torch.isinf(y).any():
        print("Warning: Label data contains NaN or infinite values")
        
except Exception as e:
    print(f"Error creating tensors: {{e}}")
    print("Please check your data types and column selections")
    exit()
'''

    # Model
    input_column_code += '''
# Model
class CustomModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(CustomModel, self).__init__()
'''

    if training_options:
        epochs = training_options.get('epochs', 10)
        batch_size = training_options.get('batchSize', 32)
        lr = training_options.get('learningRate', 0.001)
        optimizer_name = training_options.get('optimizer', 'adam').capitalize()
        loss_function = training_options.get('lossFunction', 'mse').lower()
        
        # Map loss function names to PyTorch implementations
        if loss_function == 'crossentropy' or loss_function == 'cross_entropy':
            loss_func_name = 'nn.CrossEntropyLoss()'
        elif loss_function == 'mse':
            loss_func_name = 'nn.MSELoss()'
        elif loss_function == 'bce' or loss_function == 'binary_crossentropy':
            loss_func_name = 'nn.BCELoss()'
        else:
            loss_func_name = 'nn.MSELoss()'  # Default
    else:
        epochs = 10
        batch_size = 32
        lr = 0.001
        optimizer_name = 'Adam'
        loss_func_name = 'nn.MSELoss()'

    script2_updated = script2.replace(
        'def train_model(ModelClass, input_size, output_size, epochs=20, batch_size=8, lr=0.001):',
        f'def train_model(ModelClass, input_size, output_size, epochs={epochs}, batch_size={batch_size}, lr={lr}):'
    ).replace(
        'criterion = nn.MSELoss()',
        f'criterion = {loss_func_name}'
    ).replace(
        'optimizer = optim.Adam(model.parameters(), lr=lr)',
        f'optimizer = optim.{optimizer_name}(model.parameters(), lr=lr)'
    ).replace(
        'trained_model = train_model(CustomModel, input_size, output_size, epochs=10)',
        f'trained_model = train_model(CustomModel, input_size, output_size, epochs={epochs}, batch_size={batch_size}, lr={lr})'
    )

    final_script = script1 + input_column_code + A + B + script2_updated
    return final_script
