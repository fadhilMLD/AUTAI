def topological_sort(inputt, conn):
    """Perform topological sort to get correct execution order"""
    graph = {layer['id']: [] for layer in inputt}
    in_degree = {layer['id']: 0 for layer in inputt}
    
    for connection in conn:
        graph[connection['from']].append(connection['to'])
        in_degree[connection['to']] += 1
    
    queue = []
    for layer in inputt:
        if layer['type'] == 'InputLayer' or in_degree[layer['id']] == 0:
            queue.append(layer['id'])
    
    sorted_order = []
    while queue:
        current_id = queue.pop(0)
        sorted_order.append(current_id)
        
        for neighbor in graph[current_id]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
    
    return sorted_order

def find_layer(id, dic):
    for i in dic:
        if i['id'] == id:
            return i

def calculate_layer_sizes(inputt, conn, sorted_order):
    """Calculate input and output sizes for all layers"""
    size_map = {}
    
    for layer_id in sorted_order:
        layer = find_layer(layer_id, inputt)
        
        if layer['type'] == 'InputLayer':
            size_map[layer_id] = layer['parameters']['inputShape'][0]
            
        elif layer['type'] == 'LinearLayer':
            input_connections = [c for c in conn if c['to'] == layer_id]
            
            if input_connections:
                total_input_size = 0
                for connection in input_connections:
                    prev_layer_id = connection['from']
                    prev_layer = find_layer(prev_layer_id, inputt)
                    
                    if prev_layer['type'] == 'LinearLayer':
                        total_input_size += size_map[prev_layer_id]['out']
                    elif prev_layer['type'] in ['InputLayer', 'ConcatLayer']:
                        if prev_layer_id in size_map:
                            if isinstance(size_map[prev_layer_id], dict):
                                total_input_size += size_map[prev_layer_id]['out']
                            else:
                                total_input_size += size_map[prev_layer_id]
                    else:
                        total_input_size = 64
                        break
                
                input_size = total_input_size
            else:
                input_size = 64
                
            output_size = layer['parameters']['outputSize']
            size_map[layer_id] = {
                'in': input_size,
                'out': output_size
            }
            
        elif layer['type'] == 'ConcatLayer':
            input_connections = [c for c in conn if c['to'] == layer_id]
            total_size = 0
            for connection in input_connections:
                input_layer = find_layer(connection['from'], inputt)
                if input_layer['type'] == 'LinearLayer':
                    total_size += size_map[connection['from']]['out']
                elif input_layer['type'] in ['InputLayer', 'ConcatLayer']:
                    if connection['from'] in size_map:
                        if isinstance(size_map[connection['from']], dict):
                            total_size += size_map[connection['from']]['out']
                        else:
                            total_size += size_map[connection['from']]
            size_map[layer_id] = total_size
            
        elif layer['type'] == 'OutputLayer':
            size_map[layer_id] = layer['parameters']['outputShape'][0]
    
    return size_map

def gen_evaluation_script(inputt, conn):
    """
    Generate an evaluation script that can test a trained model
    Returns the evaluation script as a string
    """
    # Find input and output layers
    input_layers = [layer for layer in inputt if layer['type'] == 'InputLayer']
    output_layers = [layer for layer in inputt if layer['type'] == 'OutputLayer']
    
    if not input_layers or not output_layers:
        return '''"""
Model Evaluation Script
Generated by Autai
ERROR: Invalid model architecture - missing input or output layers
"""
print("Error: Cannot generate evaluation script without proper model architecture")
input("Press Enter to exit...")
'''
    
    # Calculate total input size
    total_input_size = sum(layer['parameters']['inputShape'][0] for layer in input_layers)
    output_size = output_layers[0]['parameters']['outputShape'][0]
    
    # Generate the model architecture code
    sorted_order = topological_sort(inputt, conn)
    size_map = calculate_layer_sizes(inputt, conn, sorted_order)
    
    connection_map = {}
    for connection in conn:
        if connection['to'] not in connection_map:
            connection_map[connection['to']] = []
        connection_map[connection['to']].append(connection['from'])
    
    layer_vars = {}
    layer_counter = 0
    
    A = ''  # Layer definitions
    B_lines = ['    def forward(self, x):']  # Forward pass lines
    
    current_input_start = 0
    
    # Process layers in topological order to generate model architecture
    for layer_id in sorted_order:
        layer = find_layer(layer_id, inputt)
        
        if layer['type'] == 'InputLayer':
            input_size = layer['parameters']['inputShape'][0]
            var_name = f'input_{layer_counter}'
            layer_vars[layer_id] = var_name
            layer_counter += 1
            B_lines.append(f"        {var_name} = x[:, {current_input_start}:{current_input_start + input_size}]")
            current_input_start += input_size
                
        elif layer['type'] == 'LinearLayer':
            var_name = f'linear_{layer_counter}'
            layer_counter += 1
            
            if layer_id in size_map and isinstance(size_map[layer_id], dict):
                input_size = size_map[layer_id]['in']
                output_size_layer = size_map[layer_id]['out']
            else:
                input_size = layer['parameters'].get('inputSize', 64)
                output_size_layer = layer['parameters']['outputSize']
            
            A += f"        self.{var_name} = nn.Linear({input_size}, {output_size_layer})\n"
            
            if layer_id in connection_map:
                input_sources = connection_map[layer_id]
                
                if len(input_sources) == 1:
                    input_source_id = input_sources[0]
                    if input_source_id in layer_vars:
                        input_var = layer_vars[input_source_id]
                        B_lines.append(f"        {var_name}_out = self.{var_name}({input_var})")
                    else:
                        B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                else:
                    concat_vars = []
                    for input_source_id in input_sources:
                        if input_source_id in layer_vars:
                            concat_vars.append(layer_vars[input_source_id])
                        else:
                            concat_vars.append('x')
                    
                    if len(concat_vars) > 1:
                        concat_var_name = f'concat_for_{var_name}'
                        concat_vars_str = ', '.join(concat_vars)
                        B_lines.append(f"        {concat_var_name} = torch.cat([{concat_vars_str}], dim=1)")
                        B_lines.append(f"        {var_name}_out = self.{var_name}({concat_var_name})")
                    elif len(concat_vars) == 1:
                        B_lines.append(f"        {var_name}_out = self.{var_name}({concat_vars[0]})")
                    else:
                        B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                
                activation = layer['parameters'].get('activation', '')
                if activation == 'relu':
                    B_lines.append(f"        {var_name}_out = torch.relu({var_name}_out)")
                elif activation == 'sigmoid':
                    B_lines.append(f"        {var_name}_out = torch.sigmoid({var_name}_out)")
                elif activation == 'tanh':
                    B_lines.append(f"        {var_name}_out = torch.tanh({var_name}_out)")
                elif activation == 'leaky_relu':
                    B_lines.append(f"        {var_name}_out = torch.nn.functional.leaky_relu({var_name}_out)")
                
                layer_vars[layer_id] = f"{var_name}_out"
            else:
                B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                layer_vars[layer_id] = f"{var_name}_out"
                
        elif layer['type'] == 'ConcatLayer':
            var_name = f'concat_{layer_counter}'
            layer_counter += 1
            
            if layer_id in connection_map:
                input_vars = []
                for input_id in connection_map[layer_id]:
                    if input_id in layer_vars:
                        input_vars.append(layer_vars[input_id])
                    else:
                        input_vars.append('x')
                
                if len(input_vars) > 1:
                    concat_vars = ', '.join(input_vars)
                    B_lines.append(f"        {var_name} = torch.cat([{concat_vars}], dim=1)")
                    layer_vars[layer_id] = var_name
                elif len(input_vars) == 1:
                    layer_vars[layer_id] = input_vars[0]
                else:
                    layer_vars[layer_id] = 'x'
            else:
                layer_vars[layer_id] = 'x'
                
        elif layer['type'] == 'OutputLayer':
            var_name = f'output_{layer_counter}'
            layer_counter += 1
            
            input_size = 64
            if layer_id in connection_map:
                input_source_id = connection_map[layer_id][0]
                if input_source_id in size_map:
                    input_layer = find_layer(input_source_id, inputt)
                    if input_layer['type'] == 'LinearLayer':
                        input_size = size_map[input_source_id]['out']
                    elif input_layer['type'] in ['ConcatLayer', 'InputLayer']:
                        input_size = size_map[input_source_id]
            
            output_size_layer = layer['parameters']['outputShape'][0]
            A += f"        self.{var_name} = nn.Linear({input_size}, {output_size_layer})\n"

            if layer_id in connection_map:
                input_source_id = connection_map[layer_id][0]
                if input_source_id in layer_vars:
                    input_var = layer_vars[input_source_id]
                    B_lines.append(f"        {var_name}_out = self.{var_name}({input_var})")
                    
                    activation = layer['parameters'].get('activationFunction', '')
                    if output_size_layer > 1 and activation == 'softmax':
                        B_lines.append(f"        {var_name}_out = torch.softmax({var_name}_out, dim=1)")
                    elif activation == 'relu':
                        B_lines.append(f"        {var_name}_out = torch.relu({var_name}_out)")
                    elif activation == 'sigmoid':
                        B_lines.append(f"        {var_name}_out = torch.sigmoid({var_name}_out)")
                    elif activation == 'tanh':
                        B_lines.append(f"        {var_name}_out = torch.tanh({var_name}_out)")
                    
                    layer_vars[layer_id] = f"{var_name}_out"
                else:
                    B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                    layer_vars[layer_id] = f"{var_name}_out"
            else:
                B_lines.append(f"        {var_name}_out = self.{var_name}(x)")
                layer_vars[layer_id] = f"{var_name}_out"
    
    if output_layers:
        output_id = output_layers[0]['id']
        if output_id in layer_vars:
            B_lines.append(f"        return {layer_vars[output_id]}")
        else:
            B_lines.append("        return x")
    else:
        B_lines.append("        return x")
    
    B = '\n'.join(B_lines)
    
    # Build the model class definition
    model_class_def = f'''# Model Architecture (generated from your Autai model)
class CustomModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(CustomModel, self).__init__()
{A}
{B}
'''
    
    # Create custom evaluation script with the model architecture embedded
    custom_eval = f'''"""
Model Evaluation Script
Generated by Autai
Model expects input size: {total_input_size}, output size: {output_size}
"""

import torch
import torch.nn as nn
import pandas as pd
from tkinter import Tk, filedialog
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import json
import warnings
warnings.filterwarnings('ignore')

print("""
╔══════════════════════════════════════════════════╗
║          MODEL EVALUATION TOOL                   ║
║          Generated by Autai                      ║
╚══════════════════════════════════════════════════╝
""")

def select_file(title, filetypes):
    root = Tk()
    root.withdraw()
    root.attributes('-topmost', True)
    file_path = filedialog.askopenfilename(title=title, filetypes=filetypes)
    root.destroy()
    return file_path

def select_save_location():
    root = Tk()
    root.withdraw()
    root.attributes('-topmost', True)
    file_path = filedialog.asksaveasfilename(
        title="Save evaluation results as",
        defaultextension=".json",
        filetypes=[("JSON files", "*.json"), ("Text files", "*.txt"), ("All files", "*.*")]
    )
    root.destroy()
    return file_path

{model_class_def}

# Load trained model
print("\\n[1/4] Select the trained model file (.pt or .pth)")
model_path = select_file("Select trained model", [("Model files", "*.pt *.pth"), ("All files", "*.*")])
if not model_path:
    print("No model selected. Exiting...")
    exit()

try:
    if model_path.endswith('.pt'):
        model = torch.load(model_path)
        print("✓ Loaded full model (.pt file)")
    else:
        print("Loading .pth file (weights only)...")
        model = CustomModel(input_size={total_input_size}, output_size={output_size})
        model.load_state_dict(torch.load(model_path))
        print("✓ Loaded model weights (.pth file)")
    
    model.eval()
    print(f"✓ Model loaded from {{model_path}}")
except Exception as e:
    print(f"Error loading model: {{e}}")
    print("\\nTroubleshooting tips:")
    print("1. Make sure the model file is from the same architecture")
    print("2. For .pth files, ensure this evaluation script matches your training script")
    print("3. Try using the .pt file instead of .pth file")
    input("Press Enter to exit...")
    exit()

# Load test dataset
print("\\n[2/4] Select the test dataset CSV file")
csv_path = select_file("Select test CSV", [("CSV files", "*.csv"), ("All files", "*.*")])
if not csv_path:
    print("No dataset selected. Exiting...")
    exit()

df = pd.read_csv(csv_path)
print(f"✓ Dataset loaded: {{df.shape[0]}} rows, {{df.shape[1]}} columns")
print(f"Columns: {{list(df.columns)}}")

# Get input and label columns with automatic fallback
print("\\n[3/4] Specify columns")
input_cols_input = input("Enter input column names (comma separated, or press Enter for auto-select): ").strip()

if input_cols_input:
    input_cols = [col.strip() for col in input_cols_input.split(",")]
    input_cols = [col for col in input_cols if col in df.columns]
    
    if not input_cols:
        print("Warning: No valid input columns found. Auto-selecting first {total_input_size} numeric columns...")
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        input_cols = numeric_cols[:{total_input_size}]
    elif len(input_cols) < {total_input_size}:
        print(f"Warning: Only {{len(input_cols)}} valid columns found, need {total_input_size}.")
        print(f"Auto-selecting first {total_input_size} numeric columns...")
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        input_cols = numeric_cols[:{total_input_size}]
    elif len(input_cols) > {total_input_size}:
        print(f"Warning: {{len(input_cols)}} columns provided, need {total_input_size}. Using first {total_input_size}.")
        input_cols = input_cols[:{total_input_size}]
else:
    print(f"Auto-selecting first {total_input_size} numeric columns...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    input_cols = numeric_cols[:{total_input_size}]

print(f"✓ Using input columns: {{input_cols}}")

# Get label columns with automatic fallback
label_cols_input = input("Enter label column names (comma separated, or press Enter for auto-select): ").strip()

if label_cols_input:
    label_cols = [col.strip() for col in label_cols_input.split(",")]
    label_cols = [col for col in label_cols if col in df.columns]
    
    if not label_cols:
        print("Warning: No valid label columns found. Using last column as label...")
        label_cols = [df.columns[-1]]
else:
    print("Auto-selecting last column as label...")
    label_cols = [df.columns[-1]]

print(f"✓ Using label columns: {{label_cols}}")

# Check if we need one-hot encoding for labels
needs_onehot = len(label_cols) == 1 and {output_size} > 1

if needs_onehot:
    label_col_name = label_cols[0]
    unique_labels = df[label_col_name].unique()
    num_unique = len(unique_labels)
    
    print(f"\\n⚠ Detected single label column with {output_size} output neurons.")
    print(f"Found {{num_unique}} unique classes in '{{label_col_name}}': {{unique_labels}}")
    print("Applying one-hot encoding to label column...")
    
    # Encode labels to integers first
    label_encoder = LabelEncoder()
    integer_encoded = label_encoder.fit_transform(df[label_col_name])
    
    # One-hot encode
    onehot_encoder = OneHotEncoder(sparse_output=False)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))
    
    # Adjust to match expected output size
    if onehot_encoded.shape[1] > {output_size}:
        print(f"⚠ Truncating one-hot encoded output from {{onehot_encoded.shape[1]}} to {output_size} columns")
        onehot_encoded = onehot_encoded[:, :{output_size}]
    elif onehot_encoded.shape[1] < {output_size}:
        print(f"⚠ Padding one-hot encoded output from {{onehot_encoded.shape[1]}} to {output_size} columns")
        padding = np.zeros((onehot_encoded.shape[0], {output_size} - onehot_encoded.shape[1]))
        onehot_encoded = np.concatenate([onehot_encoded, padding], axis=1)
    
    # Create new DataFrame with one-hot columns
    onehot_df = pd.DataFrame(
        onehot_encoded, 
        columns=[f'{{label_col_name}}_class_{{i}}' for i in range({output_size})]
    )
    
    # Use one-hot encoded columns as labels
    label_cols = list(onehot_df.columns)
    df = pd.concat([df, onehot_df], axis=1)
    
    print(f"✓ One-hot encoding complete. New label columns: {{label_cols}}")
    print(f"Label mapping: {{dict(enumerate(label_encoder.classes_))}}")

# Prepare data
try:
    X_test = torch.tensor(df[input_cols].values, dtype=torch.float32)
    y_test = torch.tensor(df[label_cols].values, dtype=torch.float32)
    print(f"\\n✓ Test data prepared: X{{X_test.shape}}, y{{y_test.shape}}")
except Exception as e:
    print(f"Error preparing data: {{e}}")
    input("Press Enter to exit...")
    exit()

# Validate shapes
if X_test.shape[1] != {total_input_size}:
    print(f"\\n⚠ Warning: Input shape mismatch!")
    print(f"   Expected: {total_input_size}, Got: {{X_test.shape[1]}}")
    if X_test.shape[1] < {total_input_size}:
        print(f"   Padding with zeros to match expected size...")
        padding = torch.zeros((X_test.shape[0], {total_input_size} - X_test.shape[1]))
        X_test = torch.cat([X_test, padding], dim=1)
    else:
        print(f"   Truncating to match expected size...")
        X_test = X_test[:, :{total_input_size}]
    print(f"✓ Adjusted input shape: {{X_test.shape}}")

if y_test.shape[1] if len(y_test.shape) > 1 else 1 != {output_size}:
    print(f"\\n⚠ Warning: Output shape mismatch!")
    print(f"   Expected: {output_size}, Got: {{y_test.shape[1] if len(y_test.shape) > 1 else 1}}")

# Run evaluation
print("\\n[4/4] Evaluating model...")
with torch.no_grad():
    predictions = model(X_test)

# Determine task type (classification vs regression)
is_classification = len(label_cols) > 1 or (y_test.max() <= 1 and y_test.min() >= 0)

results = {{
    "model_path": model_path,
    "test_dataset": csv_path,
    "num_samples": len(X_test),
    "input_features": input_cols,
    "label_columns": label_cols
}}

if is_classification:
    # Classification metrics
    y_pred_classes = torch.argmax(predictions, dim=1).numpy()
    y_true_classes = torch.argmax(y_test, dim=1).numpy() if len(label_cols) > 1 else y_test.numpy().flatten()
    
    accuracy = accuracy_score(y_true_classes, y_pred_classes)
    
    # Handle multi-class vs binary
    average_method = 'binary' if len(np.unique(y_true_classes)) == 2 else 'weighted'
    precision = precision_score(y_true_classes, y_pred_classes, average=average_method, zero_division=0)
    recall = recall_score(y_true_classes, y_pred_classes, average=average_method, zero_division=0)
    f1 = f1_score(y_true_classes, y_pred_classes, average=average_method, zero_division=0)
    conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)
    
    results["task_type"] = "classification"
    results["metrics"] = {{
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1)
    }}
    results["confusion_matrix"] = conf_matrix.tolist()
    
    print("\\n" + "="*50)
    print("CLASSIFICATION RESULTS")
    print("="*50)
    print(f"Accuracy:  {{accuracy:.4f}}")
    print(f"Precision: {{precision:.4f}}")
    print(f"Recall:    {{recall:.4f}}")
    print(f"F1 Score:  {{f1:.4f}}")
    print(f"\\nConfusion Matrix:")
    print(conf_matrix)
    
else:
    # Regression metrics
    y_pred = predictions.numpy().flatten()
    y_true = y_test.numpy().flatten()
    
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    mae = np.mean(np.abs(y_true - y_pred))
    
    results["task_type"] = "regression"
    results["metrics"] = {{
        "mse": float(mse),
        "rmse": float(rmse),
        "mae": float(mae),
        "r2_score": float(r2)
    }}
    
    print("\\n" + "="*50)
    print("REGRESSION RESULTS")
    print("="*50)
    print(f"MSE:      {{mse:.4f}}")
    print(f"RMSE:     {{rmse:.4f}}")
    print(f"MAE:      {{mae:.4f}}")
    print(f"R² Score: {{r2:.4f}}")

# Save results
print("\\n" + "="*50)
save_path = select_save_location()
if save_path:
    with open(save_path, 'w') as f:
        json.dump(results, f, indent=4)
    print(f"✓ Evaluation results saved to: {{save_path}}")
else:
    print("Results not saved.")

print("\\nEvaluation complete!")
input("Press Enter to exit...")
'''
    
    return custom_eval
