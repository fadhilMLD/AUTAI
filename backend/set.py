
script1 = r'''
---
# Generated by Autai
# Made by Fadhil
# - Feaven Technologies - 

# Lisence - Absolutely no restriction for any kind of purpose

# Ensure you use these model for ease of development 
# without hurting or commiting any sort of criminal activity.

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
from tkinter import Tk, filedialog

def select_csv_file():
    root = Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="Select CSV file",
        filetypes=[("CSV files", "*.csv"), ("All files", "*.*")]
    )
    return file_path

# CSV
csv_file = select_csv_file()
if not csv_file:
    print("No file selected. Exiting...")
    exit()

df = pd.read_csv(csv_file)
print("\nColumns available in the dataset:\n", list(df.columns))

input_cols = input("\nEnter input column names (comma separated): ").strip().split(",")
input_cols = [col.strip() for col in input_cols]

label_cols = input("Enter label column names (comma separated): ").strip().split(",")
label_cols = [col.strip() for col in label_cols]

X = torch.tensor(df[input_cols].values, dtype=torch.float32)
y = torch.tensor(df[label_cols].values, dtype=torch.float32)

print("\nInput tensor shape:", X.shape)
print("Label tensor shape:", y.shape)

# Model
class CustomModel(nn.Module):
    def __init__(self, input_size, output_size):
        super(CustomModel, self).__init__()

'''

script2 = r'''
# Training
def train_model(ModelClass, input_size, output_size, epochs=20, batch_size=8, lr=0.001):
    dataset = TensorDataset(X, y)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = ModelClass(input_size, output_size)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        running_loss = 0.0

        for inputs, labels in loader:
            optimizer.zero_grad()
            outputs = model(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(loader):.4f}")

    print("Training complete.")
    return model

if __name__ == "__main__":
    input_size = X.shape[1]
    output_size = y.shape[1] if len(y.shape) > 1 else 1
    trained_model = train_model(CustomModel, input_size, output_size, epochs=10)

'''




def sort_by_path(data):
    from_map = {item["from"]: item for item in data}
    to_set = {item["to"] for item in data}

    start = None
    for item in data:
        if item["from"] not in to_set:
            start = item["from"]
            break
    sorted_list = []
    while start in from_map:
        current = from_map[start]
        sorted_list.append(current)
        start = current["to"]

    return sorted_list

def find_layer(id, dic):
    for i in dic:
        if i['id'] == id:
            return i


def gen_script(inputt, conn, training_options=None):
    global script1, script2

    concat_layers = []
    for i in inputt:
        if i['type'] == 'ConcatLayer':
            concat_layers.append(i)


    conn = sort_by_path(conn)

    A = ''
    B = '    def forward(self, x):\n'
    layer_iter = 0
    last_shape = None

    for i in range(len(inputt)):
        if inputt[i]['type'] == 'LinearLayer':
            A += "        " + r'self.a'+ str(layer_iter) + r' = nn.Linear('+  str(inputt[i]['parameters']['inputSize']) + r', ' + str(inputt[i]['parameters']['outputSize']) + r')' + '\n'
            last_shape = inputt[i]['parameters']['outputSize']
            inputt[i]['iter'] = 'a'+str(layer_iter)
            layer_iter += 1
        elif inputt[i]['type'] == 'OutputLayer':
            A += "        " + r'self.a'+ str(layer_iter) + r' = nn.Linear('+  str(last_shape) + r', ' + str(inputt[i]['parameters']['outputShape'][0]) + r')' + '\n'
            inputt[i]['iter'] = 'a'+str(layer_iter)
            layer_iter += 1
        elif inputt[i]['type'] == 'InputLayer':
            inputt[i]['iter'] = 'a'+str(layer_iter)
            last_shape = inputt[i]['parameters']['inputShape'][0]

    for i in conn:
        if find_layer(i['to'], inputt)['type'] == 'LinearLayer':
            B += "        " + 'x = self.' + find_layer(i['to'], inputt)['iter'] + r'(x)' + '\n'
            if find_layer(i['to'], inputt)['parameters']['activation'] == 'relu':
                B += "        " + 'x = torch.relu(x)' + '\n'
            elif find_layer(i['to'], inputt)['parameters']['activation'] == 'sigmoid':
                B += "        " + 'x = torch.sigmoid(x)' + '\n'
            elif find_layer(i['to'], inputt)['parameters']['activation'] == 'tanh':
                B += "        " + 'x = torch.tanh(x)' + '\n'
        elif find_layer(i['to'], inputt)['type'] == 'OutputLayer':
            B += "        " + 'x = self.' + find_layer(i['to'], inputt)['iter'] + r'(x)' + '\n'
            if find_layer(i['to'], inputt)['parameters']['activationFunction'] == 'relu':
                B += "        " + 'x = torch.relu(x)' + '\n'
            elif find_layer(i['to'], inputt)['parameters']['activationFunction'] == 'sigmoid':
                B += "        " + 'x = torch.sigmoid(x)' + '\n'
            elif find_layer(i['to'], inputt)['parameters']['activationFunction'] == 'tanh':
                B += "        " + 'x = torch.tanh(x)' + '\n'
            elif find_layer(i['to'], inputt)['parameters']['activationFunction'] == 'softmax':
                B += "        " + 'x = torch.softmax(x, dim=1)' + '\n'
    B += "        " + 'return x'

    # Use training options if available, otherwise use defaults
    if training_options:
        epochs = training_options.get('epochs', 10)
        batch_size = training_options.get('batchSize', 32)
        lr = training_options.get('learningRate', 0.001)
        optimizer_name = training_options.get('optimizer', 'adam').capitalize()
        loss_func_name = 'nn.CrossEntropyLoss()' if training_options.get('lossFunction') == 'crossentropy' else 'nn.MSELoss()'
    else:
        epochs = 10
        batch_size = 32
        lr = 0.001
        optimizer_name = 'Adam'
        loss_func_name = 'nn.MSELoss()'

    # Update script2 with the training parameters
    script2_updated = script2.replace(
        'def train_model(ModelClass, input_size, output_size, epochs=20, batch_size=8, lr=0.001):',
        f'def train_model(ModelClass, input_size, output_size, epochs={epochs}, batch_size={batch_size}, lr={lr}):'
    ).replace(
        'criterion = nn.MSELoss()',
        f'criterion = {loss_func_name}'
    ).replace(
        'optimizer = optim.Adam(model.parameters(), lr=lr)',
        f'optimizer = optim.{optimizer_name}(model.parameters(), lr=lr)'
    ).replace(
        'trained_model = train_model(CustomModel, input_size, output_size, epochs=10)',
        f'trained_model = train_model(CustomModel, input_size, output_size, epochs={epochs}, batch_size={batch_size}, lr={lr})'
    )


    final_script = script1 + A + B + script2_updated
    return final_script

